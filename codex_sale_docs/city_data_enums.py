import argparse
import csv
import datetime as dt
import json
import os
import shutil
from collections import Counter
from pathlib import Path
from typing import Any


DEFAULT_INPUT_CSV = Path("data/open_calgary_somervale_raw_rows_flat.csv")
DEFAULT_OUTPUT_CSV = Path("data/open_calgary_street_requested_field_dictionary.csv")
DEFAULT_OUTPUT_RST = Path("source/92_city_data_enum_dictionary.rst")
DEFAULT_RST_TABLE_DIR = Path("source/city_data/_tables/enums")
DEFAULT_DOWNLOAD_DIR = Path("source/city_data/_downloads/enums")
DEFAULT_RST_TITLE = "City Data Enumeration Dictionary"
DEFAULT_RUN_META_JSON = Path("data/open_calgary_somervale_raw_rows_meta.json")
DEFAULT_MAX_DISTINCT = 25
DEFAULT_EXPLAIN_FIELDS = [
    "land_use_designation",
    "sub_property_use",
    "year_of_construction",
    "land_size_sm",
    "land_size_sf",
    "land_size_ac",
    "comm_code",
]

DEFAULT_EXCLUDE_FIELDS = {
    "address",
    "cpid",
    "unique_key",
    "roll_number",
    "mod_date",
    "multipolygon",
    "the_geom",
    "shape",
    "x",
    "y",
    "latitude",
    "longitude",
}

OUTPUT_FIELDNAMES = [
    "field",
    "field_meaning",
    "present_count",
    "present_pct",
    "distinct_count",
    "value",
    "value_count",
    "value_pct_of_rows",
    "value_pct_of_present",
    "value_meaning",
    "meaning_source",
]

SUMMARY_TABLE_FIELDNAMES = ["Metric", "Value"]
DATA_FILE_TABLE_FIELDNAMES = ["File", "Type", "Rows", "Fields", "Download"]
URL_FIELD_TABLE_FIELDNAMES = ["URL", "Fields"]
FIELD_URL_TABLE_FIELDNAMES = ["Field", "URL Count", "URLs"]
FILE_FIELD_PROV_FIELDNAMES = ["File", "Field", "Origin"]

OUTPUT_FIELD_ORIGINS = {
    "field": "Copied from the enumerated source column name.",
    "field_meaning": "Resolved from known field descriptions or fallback field-name interpretation.",
    "present_count": "Derived count of non-empty values for the source field.",
    "present_pct": "Derived percentage of non-empty values over total input rows.",
    "distinct_count": "Derived count of distinct non-empty values for the source field.",
    "value": "Distinct value copied from the source field.",
    "value_count": "Derived count of rows containing this value.",
    "value_pct_of_rows": "Derived percentage of rows containing this value (all rows).",
    "value_pct_of_present": "Derived percentage of rows containing this value (present rows only).",
    "value_meaning": "Resolved meaning from known mappings, companion field values, or numeric derivation.",
    "meaning_source": "Resolver tag describing how value_meaning was obtained.",
}

TABLE_EXPORT_FIELD_ORIGINS = {
    "Value": "Copied from output dictionary field 'value' (blank shown as '(blank)').",
    "Count": "Copied from output dictionary field 'value_count'.",
    "%": "Copied from output dictionary field 'value_pct_of_rows'.",
    "% Present": "Copied from output dictionary field 'value_pct_of_present'.",
    "Meaning": "Copied from output dictionary field 'value_meaning'.",
    "How Resolved*": "Copied from output dictionary field 'meaning_source'.",
}

RUN_META_FIELD_ORIGINS = {
    "dataset_id": "Set by fetch script argument or default dataset id.",
    "run_id": "Generated by fetch script at runtime.",
    "captured_at": "Generated by fetch script at runtime (UTC timestamp).",
    "subject_address": "Set by fetch script argument.",
    "street_portion": "Derived by fetch script from subject address or explicit override.",
    "address_field": "Detected by fetch script from dataset schema or explicit override.",
    "detected_fields": "Detected by fetch script from dataset schema.",
    "where_clauses": "Generated by fetch script for street matching.",
    "query_urls": "Recorded by fetch script from every paginated API request.",
    "rows_raw": "Derived by fetch script as total rows returned before dedupe.",
    "rows_filtered": "Derived by fetch script after filtering (if any).",
    "rows_deduped": "Derived by fetch script after dedupe.",
    "flat_csv_fieldnames": "Derived by fetch script from flattened CSV output columns.",
    "dedupe_enabled": "Set by fetch script option state.",
}

KNOWN_FIELD_MEANINGS = {
    "assessment_class": "Assessment class code from the assessment record.",
    "assessment_class_description": "Assessment class description from the assessment record.",
    "comm_code": "Community code from the assessment record.",
    "comm_name": "Community name from the assessment record.",
    "land_size_ac": "Land size in acres from the assessment record.",
    "land_size_sf": "Land size in square feet from the assessment record.",
    "land_size_sm": "Land size in square metres from the assessment record.",
    "land_use_designation": "Land-use designation code from the assessment record.",
    "property_type": "Property type code from the assessment record.",
    "sub_property_use": "Sub-property-use code from the assessment record.",
    "year_of_construction": "Year of construction from the assessment record.",
}

KNOWN_VALUE_MEANINGS: dict[str, dict[str, str]] = {
    "assessment_class": {
        "RE": "Residential.",
        "NR": "Non-residential.",
    },
    "property_type": {
        "LI": "Land and improvements.",
        "LO": "Land only.",
    },
    "land_use_designation": {
        "M-C2": "Multi-Residential Contextual Medium Profile District.",
        "S-SPR": "Special Purpose - School, Park and Community Reserve District.",
        "S-UN": "Special Purpose - Urban Nature District.",
    },
    "sub_property_use": {
        "R201": "Primary residential condo-unit record.",
        "A004": "Ancillary V-suffix record; typically parking-related (inference).",
        "A005": "Ancillary S-suffix record; typically storage-related (inference).",
        "A006": "Auxiliary parcel/common-property record (inference).",
        "X057": "Exceptional/special record code (inference).",
    },
}


def _normalize_text(value: Any) -> str:
    if value is None:
        return ""
    return str(value).strip()


def _to_float(value: Any) -> float | None:
    raw = _normalize_text(value)
    if not raw:
        return None
    try:
        return float(raw)
    except ValueError:
        return None


def _format_pct(numerator: int, denominator: int) -> str:
    if denominator <= 0:
        return "0.00"
    return f"{(numerator / denominator) * 100:.2f}"


def _is_numeric_only(values: list[str]) -> bool:
    return bool(values) and all(_to_float(value) is not None for value in values)


def _sort_values(values: list[str]) -> list[str]:
    if _is_numeric_only(values):
        return sorted(values, key=lambda value: (float(value), value))
    return sorted(values)


def _unique_preserve(values: list[str]) -> list[str]:
    seen: set[str] = set()
    ordered: list[str] = []
    for value in values:
        key = _normalize_text(value)
        if key in seen:
            continue
        seen.add(key)
        ordered.append(key)
    return ordered


def _default_field_meaning(field: str) -> str:
    if field in KNOWN_FIELD_MEANINGS:
        return KNOWN_FIELD_MEANINGS[field]
    if field.endswith("_code"):
        return f"{field.replace('_', ' ')} from the source dataset."
    if field.endswith("_description"):
        return f"{field.replace('_', ' ')} from the source dataset."
    if field.startswith("land_size_"):
        suffix = field.split("_")[-1]
        unit_map = {"sm": "square metres", "sf": "square feet", "ac": "acres"}
        unit_label = unit_map.get(suffix, suffix)
        return f"Land size in {unit_label} from the source dataset."
    if "year" in field:
        return f"{field.replace('_', ' ')} from the source dataset."
    return f"{field.replace('_', ' ')} from the source dataset."


def _companion_fields(field: str, available_fields: set[str]) -> list[str]:
    candidates: list[str] = []
    if field.endswith("_code"):
        stem = field[:-5]
        candidates.extend([f"{stem}_name", f"{stem}_description"])
    if field == "assessment_class":
        candidates.append("assessment_class_description")
    if field == "comm_code":
        candidates.append("comm_name")
    return [value for value in _unique_preserve(candidates) if value in available_fields]


def _build_companion_value_meanings(
    *,
    rows: list[dict[str, str]],
    fields: list[str],
    available_fields: set[str],
) -> dict[tuple[str, str], tuple[str, str]]:
    resolved: dict[tuple[str, str], tuple[str, str]] = {}
    for field in fields:
        for companion in _companion_fields(field, available_fields):
            candidate_map: dict[str, set[str]] = {}
            for row in rows:
                key = _normalize_text(row.get(field))
                if not key:
                    continue
                companion_value = _normalize_text(row.get(companion))
                if not companion_value:
                    continue
                if key not in candidate_map:
                    candidate_map[key] = set()
                candidate_map[key].add(companion_value)
            for key, candidates in candidate_map.items():
                if len(candidates) != 1:
                    continue
                token = (field, key)
                if token in resolved:
                    continue
                resolved[token] = (next(iter(candidates)), f"companion:{companion}")
    return resolved


def _dynamic_value_meaning(field: str, value: str) -> tuple[str, str]:
    if not value:
        return ("Missing in source row.", "missing")
    number = _to_float(value)
    if field == "year_of_construction" and number is not None:
        year = int(number) if float(number).is_integer() else number
        return (f"Year built: {year}.", "derived")
    if field == "land_size_sm" and number is not None:
        return (f"Land parcel size value: {number} square metres.", "derived")
    if field == "land_size_sf" and number is not None:
        return (f"Land parcel size value: {number} square feet.", "derived")
    if field == "land_size_ac" and number is not None:
        return (f"Land parcel size value: {number} acres.", "derived")
    return ("", "")


def _value_meaning(
    *,
    field: str,
    value: str,
    known_map: dict[str, dict[str, str]],
    companion_map: dict[tuple[str, str], tuple[str, str]],
) -> tuple[str, str]:
    if value in known_map.get(field, {}):
        return (known_map[field][value], "known_map")
    companion = companion_map.get((field, value))
    if companion:
        return companion
    return _dynamic_value_meaning(field, value)


def _field_order(fieldnames: list[str], selected_fields: set[str]) -> list[str]:
    ordered = [field for field in fieldnames if field in selected_fields]
    extras = sorted(field for field in selected_fields if field not in set(fieldnames))
    return [*ordered, *extras]


def _slugify(value: str) -> str:
    safe = "".join(ch if ch.isalnum() else "_" for ch in value.lower()).strip("_")
    while "__" in safe:
        safe = safe.replace("__", "_")
    return safe or "field"


def _relpath_posix(path: Path, start: Path) -> str:
    return os.path.relpath(path, start).replace("\\", "/")


def _write_csv(path: Path, fieldnames: list[str], rows: list[dict[str, str]]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", newline="", encoding="utf-8") as handle:
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(rows)


def _read_json(path: Path) -> Any:
    with path.open("r", encoding="utf-8") as handle:
        return json.load(handle)


def _csv_headers(path: Path) -> list[str]:
    with path.open("r", newline="", encoding="utf-8") as handle:
        reader = csv.reader(handle)
        return next(reader, [])


def _csv_row_count(path: Path) -> int:
    with path.open("r", newline="", encoding="utf-8") as handle:
        reader = csv.reader(handle)
        next(reader, None)
        return sum(1 for _ in reader)


def _safe_copy_name(path: Path) -> str:
    parent = path.parent.name
    if parent == ".":
        return path.name
    return f"{parent}__{path.name}"


def _copy_download_file(source: Path, download_dir: Path) -> Path | None:
    if not source.exists():
        return None
    download_dir.mkdir(parents=True, exist_ok=True)
    target = download_dir / _safe_copy_name(source)
    shutil.copy2(source, target)
    return target


def _download_link(label: str, target: Path, rst_parent: Path) -> str:
    rel = _relpath_posix(target, rst_parent)
    return f":download:`{label} <{rel}>`"


def _detect_json_fields(path: Path) -> list[str]:
    payload = _read_json(path)
    if isinstance(payload, dict):
        return sorted(payload.keys())
    if isinstance(payload, list):
        keys: set[str] = set()
        for item in payload:
            if isinstance(item, dict):
                keys.update(item.keys())
        return sorted(keys)
    return []


def _detect_input_field_origin(field: str, query_urls: list[str]) -> str:
    if query_urls:
        return "Fetched from City of Calgary dataset via query_urls in fetch metadata."
    return "Fetched from City of Calgary dataset."


def _derive_default_run_meta_path(input_csv: Path) -> Path:
    text = str(input_csv)
    if text.endswith("_raw_rows_flat.csv"):
        return Path(text.replace("_raw_rows_flat.csv", "_raw_rows_meta.json"))
    return DEFAULT_RUN_META_JSON


def _load_run_meta(path: Path) -> dict[str, Any]:
    if not path.exists():
        return {}
    payload = _read_json(path)
    if isinstance(payload, dict):
        return payload
    return {}


def _summarize_file(path: Path) -> tuple[str, str]:
    if path.suffix.lower() == ".csv":
        return (str(_csv_row_count(path)), str(len(_csv_headers(path))))
    if path.suffix.lower() == ".json":
        payload = _read_json(path)
        fields = _detect_json_fields(path)
        if isinstance(payload, list):
            rows = len(payload)
            return (str(rows), str(len(fields)))
        return ("1", str(len(fields)))
    return ("", "")


def _json_list_payload_path_from_meta(meta_path: Path) -> Path:
    text = str(meta_path)
    if text.endswith("_raw_rows_meta.json"):
        return Path(text.replace("_raw_rows_meta.json", "_raw_rows.json"))
    if text.endswith("_meta.json"):
        return Path(text.replace("_meta.json", ".json"))
    return Path("")


def _field_origin_for_output(field: str) -> str:
    return OUTPUT_FIELD_ORIGINS.get(field, "Derived by enum dictionary script.")


def _field_origin_for_table_export(field: str) -> str:
    return TABLE_EXPORT_FIELD_ORIGINS.get(field, "Copied from output dictionary CSV.")


def _field_origin_for_meta(field: str) -> str:
    return RUN_META_FIELD_ORIGINS.get(field, "Recorded by fetch script runtime metadata.")


def _render_enum_rst(
    *,
    output_rst: Path,
    table_dir: Path,
    download_dir: Path,
    title: str,
    input_csv: Path,
    output_csv: Path,
    run_meta_json: Path,
    run_meta: dict[str, Any],
    input_rows: int,
    selected_fields: list[str],
    dictionary_rows: list[dict[str, str]],
) -> None:
    grouped: dict[str, list[dict[str, str]]] = {field: [] for field in selected_fields}
    for row in dictionary_rows:
        field = row.get("field", "")
        if field not in grouped:
            grouped[field] = []
        grouped[field].append(row)

    if table_dir.exists():
        shutil.rmtree(table_dir)
    table_dir.mkdir(parents=True, exist_ok=True)
    meta_table_dir = table_dir / "_meta"
    meta_table_dir.mkdir(parents=True, exist_ok=True)

    if download_dir.exists():
        shutil.rmtree(download_dir)
    download_dir.mkdir(parents=True, exist_ok=True)

    query_urls = run_meta.get("query_urls", []) if isinstance(run_meta.get("query_urls", []), list) else []
    input_fields = run_meta.get("flat_csv_fieldnames", [])
    if not isinstance(input_fields, list) or not input_fields:
        input_fields = _csv_headers(input_csv)

    generated_at = dt.datetime.now(dt.timezone.utc).strftime("%Y-%m-%d %H:%M UTC")
    summary_rows = [
        {"Metric": "Enum Report Generated At", "Value": generated_at},
        {"Metric": "Input CSV", "Value": str(input_csv)},
        {"Metric": "Output Dictionary CSV", "Value": str(output_csv)},
        {"Metric": "Input Rows", "Value": str(input_rows)},
        {"Metric": "Fields Included", "Value": str(len(selected_fields))},
        {"Metric": "Fetch Metadata JSON", "Value": str(run_meta_json) if run_meta_json.exists() else "Not found"},
        {"Metric": "Fetch Run ID", "Value": str(run_meta.get("run_id", ""))},
        {"Metric": "Fetch Captured At", "Value": str(run_meta.get("captured_at", ""))},
        {"Metric": "Dataset ID", "Value": str(run_meta.get("dataset_id", ""))},
        {"Metric": "Subject Address", "Value": str(run_meta.get("subject_address", ""))},
        {"Metric": "Street Portion", "Value": str(run_meta.get("street_portion", ""))},
        {"Metric": "Address Field", "Value": str(run_meta.get("address_field", ""))},
        {"Metric": "Rows Raw", "Value": str(run_meta.get("rows_raw", ""))},
        {"Metric": "Rows Filtered", "Value": str(run_meta.get("rows_filtered", ""))},
        {"Metric": "Rows Deduped", "Value": str(run_meta.get("rows_deduped", ""))},
        {"Metric": "Dedupe Enabled", "Value": str(run_meta.get("dedupe_enabled", ""))},
        {"Metric": "Query URL Count", "Value": str(len(query_urls))},
    ]
    summary_csv = meta_table_dir / "run_summary.csv"
    _write_csv(summary_csv, SUMMARY_TABLE_FIELDNAMES, summary_rows)

    url_field_rows: list[dict[str, str]] = []
    for url in query_urls:
        url_field_rows.append({"URL": str(url), "Fields": ", ".join(input_fields)})
    url_fields_csv = meta_table_dir / "url_field_coverage.csv"
    _write_csv(url_fields_csv, URL_FIELD_TABLE_FIELDNAMES, url_field_rows)

    field_url_rows: list[dict[str, str]] = []
    for field in input_fields:
        field_url_rows.append(
            {
                "Field": field,
                "URL Count": str(len(query_urls)),
                "URLs": "; ".join(query_urls),
            }
        )
    field_urls_csv = meta_table_dir / "field_url_coverage.csv"
    _write_csv(field_urls_csv, FIELD_URL_TABLE_FIELDNAMES, field_url_rows)

    table_fields = ["Value", "Count", "%", "% Present", "Meaning", "How Resolved*"]
    generated_enum_table_paths: list[Path] = []

    for field in selected_fields:
        field_rows = grouped.get(field, [])
        if not field_rows:
            continue
        table_rows: list[dict[str, str]] = []
        for row in field_rows:
            value = row.get("value", "")
            table_rows.append(
                {
                    "Value": value if value else "(blank)",
                    "Count": row.get("value_count", ""),
                    "%": row.get("value_pct_of_rows", ""),
                    "% Present": row.get("value_pct_of_present", ""),
                    "Meaning": row.get("value_meaning", "") or "No resolved meaning.",
                    "How Resolved*": row.get("meaning_source", "") or "unresolved",
                }
            )

        table_path = table_dir / f"{_slugify(field)}.csv"
        _write_csv(table_path, table_fields, table_rows)
        generated_enum_table_paths.append(table_path)

    file_field_rows: list[dict[str, str]] = []
    for field in _csv_headers(input_csv):
        file_field_rows.append(
            {
                "File": str(input_csv),
                "Field": field,
                "Origin": _detect_input_field_origin(field, query_urls),
            }
        )

    for field in _csv_headers(output_csv):
        file_field_rows.append(
            {
                "File": str(output_csv),
                "Field": field,
                "Origin": _field_origin_for_output(field),
            }
        )

    if run_meta_json.exists():
        for field in _detect_json_fields(run_meta_json):
            file_field_rows.append(
                {
                    "File": str(run_meta_json),
                    "Field": field,
                    "Origin": _field_origin_for_meta(field),
                }
            )

    raw_rows_json = _json_list_payload_path_from_meta(run_meta_json)
    if raw_rows_json and raw_rows_json.exists():
        for field in _detect_json_fields(raw_rows_json):
            file_field_rows.append(
                {
                    "File": str(raw_rows_json),
                    "Field": field,
                    "Origin": "Fetched from City of Calgary dataset via query_urls in fetch metadata.",
                }
            )

    for table_path in sorted(generated_enum_table_paths):
        for field in _csv_headers(table_path):
            file_field_rows.append(
                {
                    "File": str(table_path),
                    "Field": field,
                    "Origin": _field_origin_for_table_export(field),
                }
            )

    file_field_csv = meta_table_dir / "data_file_field_provenance.csv"
    _write_csv(file_field_csv, FILE_FIELD_PROV_FIELDNAMES, file_field_rows)

    data_files: list[tuple[str, Path, str]] = [
        ("Input Flat CSV", input_csv, "csv"),
        ("Enumeration Dictionary CSV", output_csv, "csv"),
        ("Run Summary CSV", summary_csv, "csv"),
        ("URL->Field Coverage CSV", url_fields_csv, "csv"),
        ("Field->URL Coverage CSV", field_urls_csv, "csv"),
        ("Data File Field Provenance CSV", file_field_csv, "csv"),
    ]
    if run_meta_json.exists():
        data_files.append(("Fetch Run Metadata JSON", run_meta_json, "json"))
    if raw_rows_json and raw_rows_json.exists():
        data_files.append(("Raw Rows JSON", raw_rows_json, "json"))
    for table_path in sorted(generated_enum_table_paths):
        data_files.append((f"Enum Table CSV ({table_path.stem})", table_path, "csv"))

    data_file_rows: list[dict[str, str]] = []
    download_lines: list[str] = []
    for label, path, file_type in data_files:
        copied = _copy_download_file(path, download_dir)
        if copied is None:
            continue
        rows_count, field_count = _summarize_file(path)
        data_file_rows.append(
            {
                "File": str(path),
                "Type": file_type,
                "Rows": rows_count,
                "Fields": field_count,
                "Download": _download_link("Download", copied, output_rst.parent),
            }
        )
        download_lines.append(f"- {label}: {_download_link(path.name, copied, output_rst.parent)}")

    data_files_csv = meta_table_dir / "data_file_inventory.csv"
    _write_csv(data_files_csv, DATA_FILE_TABLE_FIELDNAMES, data_file_rows)

    lines = [title, "=" * len(title), ""]
    lines.append("Enumerations and code mappings derived from the fetched city-data CSV.")
    lines.append("")
    lines.append(".. contents::")
    lines.append("   :local:")
    lines.append("   :depth: 2")
    lines.append("")
    lines.append("``How Resolved*`` footnote: [#how-resolved]_.")
    lines.append("")

    for field in selected_fields:
        field_rows = grouped.get(field, [])
        if not field_rows:
            continue
        heading = f"``{field}``"
        lines.append(heading)
        lines.append("-" * len(heading))
        lines.append("")
        first = field_rows[0]
        lines.append(f"- Meaning: {first.get('field_meaning', '') or 'No field meaning available.'}")
        lines.append(
            f"- Present: ``{first.get('present_count', '0')}/{input_rows}``"
            f" (``{first.get('present_pct', '0.00')}%``)"
        )
        lines.append(f"- Distinct non-empty values: ``{first.get('distinct_count', '0')}``")
        lines.append("")

        table_path = table_dir / f"{_slugify(field)}.csv"
        lines.append(f".. csv-table:: Enumerations for ``{field}``")
        lines.append(f"   :file: {_relpath_posix(table_path, output_rst.parent)}")
        lines.append("   :header-rows: 1")
        lines.append("   :class: enum-table-angled")
        lines.append("   :widths: auto")
        lines.append("")

    lines.append("Last Run Metadata")
    lines.append("-----------------")
    lines.append("")
    lines.append(".. csv-table:: Run Summary")
    lines.append(f"   :file: {_relpath_posix(summary_csv, output_rst.parent)}")
    lines.append("   :header-rows: 1")
    lines.append("   :widths: auto")
    lines.append("")

    lines.append("Data Files")
    lines.append("^^^^^^^^^^")
    lines.append("")
    if download_lines:
        lines.extend(download_lines)
    else:
        lines.append("- No files available for download.")
    lines.append("")
    lines.append(".. csv-table:: Data File Inventory")
    lines.append(f"   :file: {_relpath_posix(data_files_csv, output_rst.parent)}")
    lines.append("   :header-rows: 1")
    lines.append("   :widths: auto")
    lines.append("")

    lines.append("URLs Visited And Field Coverage")
    lines.append("^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^")
    lines.append("")
    if query_urls:
        lines.append(".. csv-table:: URLs Visited And Retrieved Fields")
        lines.append(f"   :file: {_relpath_posix(url_fields_csv, output_rst.parent)}")
        lines.append("   :header-rows: 1")
        lines.append("   :widths: auto")
        lines.append("")
        lines.append(".. csv-table:: Field To URL Mapping")
        lines.append(f"   :file: {_relpath_posix(field_urls_csv, output_rst.parent)}")
        lines.append("   :header-rows: 1")
        lines.append("   :widths: auto")
        lines.append("")
    else:
        lines.append("- No query URLs found in fetch metadata JSON.")
        lines.append("")

    lines.append("Data File Fields And Origins")
    lines.append("^^^^^^^^^^^^^^^^^^^^^^^^^^^^")
    lines.append("")
    lines.append(".. csv-table:: Data File Field Provenance")
    lines.append(f"   :file: {_relpath_posix(file_field_csv, output_rst.parent)}")
    lines.append("   :header-rows: 1")
    lines.append("   :widths: auto")
    lines.append("")

    lines.append("Explanatory Data Dictionary")
    lines.append("---------------------------")
    lines.append("")
    lines.append("- ``Count``: number of rows containing the value.")
    lines.append("- ``%``: percentage of all rows containing the value (formerly ``value_pct_of_rows``).")
    lines.append("- ``% Present``: percentage among non-blank rows for the field (formerly ``value_pct_of_present``).")
    lines.append("- ``How Resolved*``: resolver tag for how ``Meaning`` was obtained; see [#how-resolved]_.")
    lines.append("")
    lines.append("Resolver Tags")
    lines.append("^^^^^^^^^^^^^")
    lines.append("")
    lines.append("- ``known_map``: meaning comes from explicit code mapping hardcoded in the script.")
    lines.append("- ``companion:<field>``: meaning is copied from a paired descriptor field in the same source row.")
    lines.append("  Example: ``comm_code=SOM`` resolves as ``companion:comm_name`` because ``comm_name=SOMERSET``.")
    lines.append("- ``derived``: meaning generated from numeric context (for example year or land-size units).")
    lines.append("- ``missing``: source value was blank.")
    lines.append("")
    lines.append(".. [#how-resolved] ``How Resolved*`` indicates the rule used to populate ``Meaning`` for each value.")
    lines.append("   It does not indicate confidence; it indicates transformation provenance.")
    lines.append("")

    output_rst.parent.mkdir(parents=True, exist_ok=True)
    output_rst.write_text("\n".join(lines), encoding="utf-8")


def _select_fields(
    *,
    rows: list[dict[str, str]],
    fieldnames: list[str],
    max_distinct: int,
    include_numeric: bool,
    auto_fields: bool,
    include_fields: list[str],
    exclude_fields: set[str],
) -> list[str]:
    include_tokens = {_normalize_text(value) for value in include_fields if _normalize_text(value)}
    if include_tokens:
        return _field_order(fieldnames, include_tokens - exclude_fields)

    if not auto_fields:
        include_tokens = set(DEFAULT_EXPLAIN_FIELDS)
        return _field_order(fieldnames, include_tokens - exclude_fields)

    selected: set[str] = set()
    for field in fieldnames:
        if field in exclude_fields:
            continue
        values = [_normalize_text(row.get(field, "")) for row in rows]
        present = [value for value in values if value]
        if not present:
            continue
        if (not include_numeric) and _is_numeric_only(present):
            continue
        distinct = len(set(present))
        if distinct <= max_distinct:
            selected.add(field)
    return _field_order(fieldnames, selected)


def build_enum_dictionary(
    *,
    input_csv: Path,
    output_csv: Path,
    output_rst: Path | None,
    rst_table_dir: Path,
    download_dir: Path,
    rst_title: str,
    run_meta_json: Path,
    max_distinct: int,
    include_numeric: bool,
    auto_fields: bool,
    include_fields: list[str],
    exclude_fields: set[str],
    include_blank: bool,
    debug: bool,
) -> dict[str, Any]:
    if not input_csv.exists():
        raise RuntimeError(f"Missing input CSV: {input_csv}")
    with input_csv.open("r", newline="", encoding="utf-8") as handle:
        reader = csv.DictReader(handle)
        fieldnames = list(reader.fieldnames or [])
        rows = list(reader)
    if not rows:
        raise RuntimeError(f"No rows found in {input_csv}")

    selected_fields = _select_fields(
        rows=rows,
        fieldnames=fieldnames,
        max_distinct=max_distinct,
        include_numeric=include_numeric,
        auto_fields=auto_fields,
        include_fields=include_fields,
        exclude_fields=exclude_fields,
    )
    if not selected_fields:
        raise RuntimeError("No enumerated fields matched the current selection rules.")

    available_fields = set(fieldnames)
    companion_meanings = _build_companion_value_meanings(
        rows=rows,
        fields=selected_fields,
        available_fields=available_fields,
    )

    total_rows = len(rows)
    dictionary_rows: list[dict[str, str]] = []
    for field in selected_fields:
        values = [_normalize_text(row.get(field, "")) for row in rows]
        present = [value for value in values if value]
        present_count = len(present)
        counts = Counter(present)
        sorted_values = _sort_values(list(counts.keys()))
        field_meaning = _default_field_meaning(field)

        if debug:
            print(
                f"DEBUG: field={field} present={present_count}/{total_rows} "
                f"distinct={len(sorted_values)}"
            )

        for value in sorted_values:
            value_meaning, meaning_source = _value_meaning(
                field=field,
                value=value,
                known_map=KNOWN_VALUE_MEANINGS,
                companion_map=companion_meanings,
            )
            count = counts[value]
            dictionary_rows.append(
                {
                    "field": field,
                    "field_meaning": field_meaning,
                    "present_count": str(present_count),
                    "present_pct": _format_pct(present_count, total_rows),
                    "distinct_count": str(len(sorted_values)),
                    "value": value,
                    "value_count": str(count),
                    "value_pct_of_rows": _format_pct(count, total_rows),
                    "value_pct_of_present": _format_pct(count, present_count),
                    "value_meaning": value_meaning,
                    "meaning_source": meaning_source,
                }
            )

        missing_count = total_rows - present_count
        if include_blank and missing_count > 0:
            value_meaning, meaning_source = _value_meaning(
                field=field,
                value="",
                known_map=KNOWN_VALUE_MEANINGS,
                companion_map=companion_meanings,
            )
            dictionary_rows.append(
                {
                    "field": field,
                    "field_meaning": field_meaning,
                    "present_count": str(present_count),
                    "present_pct": _format_pct(present_count, total_rows),
                    "distinct_count": str(len(sorted_values)),
                    "value": "",
                    "value_count": str(missing_count),
                    "value_pct_of_rows": _format_pct(missing_count, total_rows),
                    "value_pct_of_present": "",
                    "value_meaning": value_meaning,
                    "meaning_source": meaning_source,
                }
            )

    output_csv.parent.mkdir(parents=True, exist_ok=True)
    with output_csv.open("w", newline="", encoding="utf-8") as handle:
        writer = csv.DictWriter(handle, fieldnames=OUTPUT_FIELDNAMES)
        writer.writeheader()
        writer.writerows(dictionary_rows)

    if output_rst is not None:
        run_meta = _load_run_meta(run_meta_json)
        _render_enum_rst(
            output_rst=output_rst,
            table_dir=rst_table_dir,
            download_dir=download_dir,
            title=rst_title,
            input_csv=input_csv,
            output_csv=output_csv,
            run_meta_json=run_meta_json,
            run_meta=run_meta,
            input_rows=total_rows,
            selected_fields=selected_fields,
            dictionary_rows=dictionary_rows,
        )

    return {
        "input_csv": str(input_csv),
        "output_csv": str(output_csv),
        "output_rst": str(output_rst) if output_rst is not None else "",
        "run_meta_json": str(run_meta_json),
        "input_rows": total_rows,
        "fields_selected": len(selected_fields),
        "dictionary_rows": len(dictionary_rows),
        "selected_fields": selected_fields,
    }


def _build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description=(
            "Build an enumeration dictionary from fetched city-data CSV: "
            "field -> distinct values -> counts -> human-readable meanings."
        )
    )
    parser.add_argument("--input-csv", default=str(DEFAULT_INPUT_CSV), help="Input flat city-data CSV path.")
    parser.add_argument("--output-csv", default=str(DEFAULT_OUTPUT_CSV), help="Output dictionary CSV path.")
    parser.add_argument(
        "--output-rst",
        default=str(DEFAULT_OUTPUT_RST),
        help="Output formatted rST summary path.",
    )
    parser.add_argument(
        "--rst-table-dir",
        default=str(DEFAULT_RST_TABLE_DIR),
        help="Directory for per-field CSV tables referenced by the rST page.",
    )
    parser.add_argument(
        "--download-dir",
        default=str(DEFAULT_DOWNLOAD_DIR),
        help="Directory for downloadable file copies referenced from the rST page.",
    )
    parser.add_argument(
        "--run-meta-json",
        default="",
        help="Fetch-run metadata JSON path. Defaults by deriving from --input-csv or fallback default path.",
    )
    parser.add_argument("--rst-title", default=DEFAULT_RST_TITLE, help="Title for the generated rST page.")
    parser.add_argument(
        "--max-distinct",
        type=int,
        default=DEFAULT_MAX_DISTINCT,
        help="When --auto-fields is enabled, include fields with <= this many distinct non-empty values.",
    )
    parser.add_argument(
        "--include-numeric",
        action="store_true",
        help="When --auto-fields is enabled, also include numeric fields that match --max-distinct.",
    )
    parser.add_argument(
        "--auto-fields",
        action="store_true",
        help="Use automatic field discovery instead of the default explain-field set.",
    )
    parser.add_argument(
        "--field",
        action="append",
        default=[],
        help="Explicit field(s) to include (repeatable). Overrides default field set.",
    )
    parser.add_argument(
        "--exclude-field",
        action="append",
        default=[],
        help="Exclude field(s) from output (repeatable).",
    )
    parser.add_argument(
        "--include-blank",
        action="store_true",
        help="Include an explicit blank-value row when a field has missing values.",
    )
    parser.add_argument("--no-rst", action="store_true", help="Skip rST page generation.")
    parser.add_argument("--debug", action="store_true", help="Print field-level selection debug output.")
    return parser


def main() -> int:
    args = _build_parser().parse_args()
    run_meta_json = (
        Path(args.run_meta_json)
        if _normalize_text(args.run_meta_json)
        else _derive_default_run_meta_path(Path(args.input_csv))
    )
    exclude_fields = set(DEFAULT_EXCLUDE_FIELDS)
    exclude_fields.update(
        {_normalize_text(value) for value in args.exclude_field if _normalize_text(value)}
    )
    try:
        result = build_enum_dictionary(
            input_csv=Path(args.input_csv),
            output_csv=Path(args.output_csv),
            output_rst=None if args.no_rst else Path(args.output_rst),
            rst_table_dir=Path(args.rst_table_dir),
            download_dir=Path(args.download_dir),
            rst_title=args.rst_title,
            run_meta_json=run_meta_json,
            max_distinct=max(1, int(args.max_distinct)),
            include_numeric=bool(args.include_numeric),
            auto_fields=bool(args.auto_fields),
            include_fields=list(args.field),
            exclude_fields=exclude_fields,
            include_blank=bool(args.include_blank),
            debug=bool(args.debug),
        )
    except Exception as exc:  # pragma: no cover - CLI guard
        print(f"ERROR: {exc}")
        return 1

    print(f"Input CSV: {result['input_csv']}")
    print(f"Input rows: {result['input_rows']}")
    print(f"Selected fields: {result['fields_selected']}")
    print(f"Dictionary rows: {result['dictionary_rows']}")
    print(f"Wrote enumeration dictionary: {result['output_csv']}")
    print(f"Run metadata JSON used: {result['run_meta_json']}")
    if result["output_rst"]:
        print(f"Wrote formatted rST page: {result['output_rst']}")
    if os.environ.get("DEBUG_ENUM_FIELDS", ""):
        print("Fields:")
        for field in result["selected_fields"]:
            print(f"- {field}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
